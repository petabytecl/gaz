---
phase: 14-workers
plan: 02
type: execute
wave: 2
depends_on: ["14-01"]
files_modified:
  - worker/supervisor.go
  - worker/manager.go
  - worker/errors.go
autonomous: true

must_haves:
  truths:
    - "Supervisor wraps workers with panic recovery"
    - "Panics are logged with stack traces and worker is restarted"
    - "Circuit breaker stops restarts after max failures in time window"
    - "WorkerManager tracks and coordinates multiple workers"
  artifacts:
    - path: "worker/supervisor.go"
      provides: "Per-worker supervision with panic recovery and restart"
      exports: ["supervisor"]
      contains: "defer func() { if r := recover()"
    - path: "worker/manager.go"
      provides: "WorkerManager for tracking multiple workers"
      exports: ["Manager", "NewManager"]
      contains: "type Manager struct"
    - path: "worker/errors.go"
      provides: "Worker-specific errors"
      exports: ["ErrCircuitBreakerTripped", "ErrWorkerStopped"]
  key_links:
    - from: "worker/supervisor.go"
      to: "worker/backoff.go"
      via: "uses backoff for restart delays"
      pattern: "NewBackoff|backoff\\.Duration"
    - from: "worker/manager.go"
      to: "worker/supervisor.go"
      via: "creates supervisors for each worker"
      pattern: "supervisor|supervise"
    - from: "worker/supervisor.go"
      to: "runtime/debug"
      via: "captures stack trace on panic"
      pattern: "debug\\.Stack"
---

<objective>
Implement WorkerManager and Supervisor with panic recovery, exponential backoff restarts, and circuit breaker.

Purpose: Core supervision logic that ensures workers are resilient - panics don't crash the app, failed workers restart with backoff, and runaway failures are stopped by circuit breaker.
Output: manager.go and supervisor.go implementing the supervision tree pattern.
</objective>

<execution_context>
@~/.config/opencode/get-shit-done/workflows/execute-plan.md
@~/.config/opencode/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/14-workers/14-CONTEXT.md
@.planning/phases/14-workers/14-RESEARCH.md
@.planning/phases/14-workers/14-01-SUMMARY.md

# Prior plan output
@worker/worker.go
@worker/options.go
@worker/backoff.go

# Existing patterns
@app.go
@di/service.go
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Supervisor with panic recovery and circuit breaker</name>
  <files>worker/supervisor.go, worker/errors.go</files>
  <action>
**worker/errors.go:**
- Define sentinel errors with 'worker:' prefix:
  - `ErrCircuitBreakerTripped` - Worker exhausted restarts
  - `ErrWorkerStopped` - Worker stopped normally
  - `ErrCriticalWorkerFailed` - Critical worker failed (triggers app shutdown)

**worker/supervisor.go:**
Create internal `supervisor` struct that wraps a single worker:

```go
type supervisor struct {
    worker     Worker
    opts       *WorkerOptions
    backoff    *backoff.Backoff
    logger     *slog.Logger
    
    // Circuit breaker state
    failures    int
    windowStart time.Time
    
    // Lifecycle
    ctx        context.Context
    cancel     context.CancelFunc
    done       chan struct{}
    
    // Callback for critical worker failure
    onCriticalFail func()
}
```

Implement `supervise()` method with:
1. **Main supervision loop** - Runs until context cancelled
2. **Panic recovery** - Use `defer recover()` to catch panics
3. **Stack trace capture** - Use `runtime/debug.Stack()` on panic
4. **Structured logging** - Log start, stop, panic, restart events with worker name
5. **Exponential backoff** - Use BackoffConfig for restart delays
6. **Circuit breaker** - Track failures within CircuitWindow, trip after MaxRestarts
7. **Stable run detection** - Reset backoff after StableRunPeriod of clean running
8. **Critical worker handling** - Call onCriticalFail callback if critical and circuit trips

The supervisor calls worker.Start() then monitors. When context is cancelled or Stop() is called, it calls worker.Stop() and waits for clean exit.

Key patterns from RESEARCH.md:
- Each worker gets scoped logger with `slog.String("worker", w.Name())`
- Jitter in backoff prevents thundering herd
- Circuit breaker resets window after windowStart + CircuitWindow passes

**Context propagation for graceful shutdown (WRK-04):**
- Supervisor creates a child context from manager's context
- When manager.Stop() cancels the parent context, supervisor's context is also cancelled
- Supervisor then calls worker.Stop() and waits for clean exit

**Structured logging with slog (WRK-07):**
- Use `slog.Error()` for panic recovery with stack trace attribute
- Use `slog.Info()` for lifecycle events (start, stop, restart)
- Use `slog.Warn()` for circuit breaker warnings
  </action>
  <verify>`go build ./worker/...` succeeds</verify>
  <done>Supervisor handles panics, logs with stack trace, restarts with backoff, trips circuit breaker after max failures</done>
</task>

<task type="auto">
  <name>Task 2: Create WorkerManager for coordinating workers</name>
  <files>worker/manager.go</files>
  <action>
Create `Manager` struct that coordinates multiple workers:

```go
type Manager struct {
    logger      *slog.Logger
    supervisors []*supervisor
    
    mu          sync.Mutex
    running     bool
    ctx         context.Context
    cancel      context.CancelFunc
    wg          sync.WaitGroup
    
    // Callback for critical worker failure (signals app shutdown)
    onCriticalFail func()
}
```

Implement methods:

**NewManager(logger *slog.Logger) *Manager**
- Creates new manager with logger
- Initializes empty supervisors slice

**Register(w Worker, opts ...WorkerOption) error**
- Applies options to create WorkerOptions
- Creates supervisor for worker (or N supervisors for pool)
- For pools: Create wrapper workers with indexed names ("worker-1", "worker-2", etc.)
- Returns error if manager already running

**Start(ctx context.Context) error**
- Sets running = true
- Creates manager context with cancel
- Spawns goroutine for each supervisor (all parallel, per CONTEXT.md)
- Returns immediately (non-blocking)

**Stop() error**
- Cancels context
- Waits for all supervisor goroutines to complete (wg.Wait())
- Returns first error encountered (or nil)

**Done() <-chan struct{}**
- Returns channel that closes when all workers have stopped
- Useful for external shutdown verification (WRK-06)

**SetCriticalFailHandler(fn func())**
- Sets callback for when a critical worker's circuit breaker trips
- Used by App to trigger graceful shutdown

Key patterns:
- All workers start concurrently (no ordering)
- Manager owns context lifecycle
- WaitGroup ensures all supervisors complete before Done closes
  </action>
  <verify>`go build ./worker/...` succeeds</verify>
  <done>WorkerManager can register workers (including pools), start all concurrently, stop gracefully, and notify on critical failures</done>
</task>

</tasks>

<verification>
- [ ] `go build ./worker/...` compiles without errors
- [ ] Supervisor has recover/panic handling with debug.Stack()
- [ ] Supervisor uses backoff.Duration() for restart delays
- [ ] Supervisor tracks failures and trips circuit breaker
- [ ] Manager.Register() supports pool workers with indexed names
- [ ] Manager.Start() spawns all workers concurrently
- [ ] Manager.Stop() waits for all workers to complete
- [ ] Manager.Done() returns a channel for shutdown verification
</verification>

<success_criteria>
- Panics in workers are recovered and logged with stack trace
- Workers restart with exponential backoff (1s, 2s, 4s... up to 5m)
- Circuit breaker trips after 5 failures in 10 minutes (configurable)
- Critical workers trigger onCriticalFail callback when circuit trips
- Pool workers get indexed names ("worker-1", "worker-2", etc.)
- All workers start and stop concurrently
</success_criteria>

<output>
After completion, create `.planning/phases/14-workers/14-02-SUMMARY.md`
</output>
