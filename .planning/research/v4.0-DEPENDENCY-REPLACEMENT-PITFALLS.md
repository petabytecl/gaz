# Dependency Replacement Pitfalls

**Milestone:** v4.0 Dependency Replacement
**Dependencies:** jpillora/backoff, robfig/cron/v3, lmittmann/tint, alexliesenfeld/health
**Researched:** 2026-02-01
**Confidence:** HIGH (based on code analysis + ecosystem research)

---

## Critical Pitfalls

Mistakes that cause production incidents or require rewrites.

---

### Pitfall 1: DST Edge Cases in cron/internal

**What goes wrong:** The internal `cron/internal` implementation handles Daylight Saving Time transitions, but edge cases may differ from `robfig/cron` behavior in subtle ways.

**Why it happens:** DST handling is notoriously complex:
- **Spring Forward (Missing Hour):** Jobs scheduled for 2:30 AM when clocks jump 2:00 → 3:00 never fire if not handled
- **Fall Back (Repeated Hour):** Jobs scheduled for 1:30 AM during fall-back may fire twice
- **Sao Paulo edge case:** Some timezones have DST at midnight, making midnight invalid (robfig/cron issue #157)

**Evidence from codebase:** The `_tmp_trust/cron/internal/spec_test.go` already tests DST cases (lines 113-186), including the Sao Paulo edge case from robfig/cron#157 and issue #144 (slash-0 hang). This indicates awareness but not complete coverage.

**Consequences:**
- Jobs skip execution during spring DST transition
- Jobs double-fire during fall DST transition
- Production cron jobs silently fail for users in affected timezones

**Warning signs:**
- Tests pass locally (using system timezone) but fail in CI (using UTC)
- Jobs scheduled between 2-3 AM behave inconsistently
- Users in Brazil/Sao Paulo, Chile, or similar report missed jobs

**Prevention:**
```go
// Add comprehensive DST tests covering:
// 1. Spring forward: 2:30 AM job on DST transition day
// 2. Fall back: 1:30 AM job on DST transition day  
// 3. Midnight DST (Sao Paulo pattern)
// 4. CRON_TZ prefix handling

func TestDSTSpringForward(t *testing.T) {
    tz, _ := time.LoadLocation("America/New_York")
    // Test 2:30 AM job on March 10, 2024 (DST starts)
    spec := "TZ=America/New_York 0 30 2 10 Mar ?"
    // Should fire at 3:00 AM EDT, not skip entirely
}
```

**Phase to address:** Phase 1 (cron/internal replacement) - Block merge until DST tests comprehensive

---

### Pitfall 2: Health Check Cascade Failure

**What goes wrong:** Replacing `alexliesenfeld/health` with internal `healthz` could change timeout or status code behavior, causing Kubernetes to kill pods or remove them from rotation.

**Why it happens:** The current implementation uses alexliesenfeld/health features:
- `WithStatusCodeDown(http.StatusOK)` for liveness (200 even on failure)
- `WithStatusCodeDown(http.StatusServiceUnavailable)` for readiness (503 on failure)
- `IETFResultWriter` for `application/health+json` format

**Evidence from codebase:** 
```go
// handlers.go - Current behavior
func (m *Manager) NewLivenessHandler() http.Handler {
    return health.NewHandler(checker,
        health.WithStatusCodeDown(http.StatusOK), // 200 on failure!
    )
}
```

If internal implementation doesn't replicate this exact behavior, liveness failures return 503, triggering pod restarts.

**Consequences:**
- Liveness returns 503 instead of 200 → Kubernetes kills pods
- Readiness check timeout changes → cascading traffic redistribution
- Pod restart storm during degraded database → complete outage

**Warning signs:**
- Increased pod restart count after deployment
- `kubectl describe pod` shows liveness probe failures
- Health endpoint response format changed (missing IETF fields)

**Prevention:**
1. Create behavior characterization tests BEFORE replacement:
```go
func TestLivenessReturns200OnFailure(t *testing.T) {
    // Current alexliesenfeld/health behavior
    m := NewManager()
    m.AddLivenessCheck("always-fail", func(ctx context.Context) error {
        return errors.New("always fail")
    })
    handler := m.NewLivenessHandler()
    
    rec := httptest.NewRecorder()
    handler.ServeHTTP(rec, httptest.NewRequest("GET", "/", nil))
    
    assert.Equal(t, http.StatusOK, rec.Code) // Must be 200, not 503
}

func TestIETFResponseFormat(t *testing.T) {
    // Test Content-Type: application/health+json
    // Test JSON structure matches IETF draft
}
```

2. Run both old and new handlers in shadow mode comparing responses

**Phase to address:** Phase 2 (health replacement) - Require characterization tests before ANY code changes

---

### Pitfall 3: Backoff Overflow to Negative

**What goes wrong:** The `jpillora/backoff` library handles integer overflow; a naive replacement may not.

**Why it happens:** Go's `time.Duration` is `int64` nanoseconds. Calculating `base * 2^n` overflows when n > 30ish, wrapping to negative.

**Evidence from codebase:** The existing internal backoff in `_tmp_trust/srex/backoff/exponential_test.go` has `TestBackOffOverflow`:
```go
func TestBackOffOverflow(t *testing.T) {
    var (
        testInitialInterval time.Duration = math.MaxInt64 / 2
        testMultiplier                    = 2.1
    )
    // Tests that overflow clamps to MaxInterval
}
```

But the current `worker/backoff.go` uses `jpillora/backoff` directly:
```go
func (c *BackoffConfig) NewBackoff() *backoff.Backoff {
    return &backoff.Backoff{Min: c.Min, Max: c.Max, ...}
}
```

**Consequences:**
- After ~30 attempts, backoff becomes negative
- Worker restarts immediately in infinite loop
- CPU spins to 100%, memory exhaustion

**Warning signs:**
- Worker restarts increasingly fast instead of slowing down
- After ~30 consecutive failures, behavior changes dramatically
- CPU spike in worker supervision

**Prevention:**
```go
// Internal implementation must check for overflow
func (b *Backoff) Duration() time.Duration {
    d := b.base * time.Duration(1<<b.attempt)
    
    // Overflow protection: if result is negative or exceeds max, clamp
    if d <= 0 || d > b.max {
        return b.max
    }
    return d
}
```

**Phase to address:** Phase 1 (backoff replacement) - Add overflow tests matching jpillora/backoff behavior

---

### Pitfall 4: Jitter Implementation Race Condition

**What goes wrong:** Adding jitter to backoff requires a random number generator. Using a shared `rand.Source` without locking causes data races.

**Why it happens:** Pre-Go 1.22, `math/rand` sources are not thread-safe. Multiple workers getting backoff delays concurrently corrupt the PRNG state.

**Current code pattern:**
```go
// worker/supervisor.go uses backoff concurrently across workers
func (s *supervisor) supervise() {
    delay := s.backoff.Duration() // Called from multiple goroutines
}
```

**Consequences:**
- `-race` detector fails in CI
- Rarely: corrupted random values, identical delays across workers
- Thundering herd: all workers retry simultaneously

**Warning signs:**
- `go test -race` failures mentioning rand
- Workers restarting at exactly the same time after failures
- Sporadic test flakes related to timing

**Prevention:**
```go
// Option 1: Use Go 1.22+ math/rand/v2 (globally safe)
import "math/rand/v2"

func jitteredDuration(base time.Duration) time.Duration {
    jitter := rand.Int64N(int64(base) / 10) // Thread-safe in rand/v2
    return base + time.Duration(jitter)
}

// Option 2: Per-instance rand.Rand with mutex
type Backoff struct {
    mu   sync.Mutex
    rng  *rand.Rand
}
```

**Phase to address:** Phase 1 (backoff replacement) - Use rand/v2 or document concurrency requirement

---

## Moderate Pitfalls

Mistakes that cause delays or technical debt.

---

### Pitfall 5: slog Handler WithAttrs/WithGroup Ignored

**What goes wrong:** Custom slog handlers must properly implement `WithAttrs` and `WithGroup`. A minimal tint replacement often ignores these.

**Why it happens:** The slog interface requires these methods:
```go
type Handler interface {
    Enabled(context.Context, Level) bool
    Handle(context.Context, Record) error
    WithAttrs(attrs []Attr) Handler  // MUST return new handler
    WithGroup(name string) Handler   // MUST return new handler
}
```

Naive implementations return `h` (self) instead of a copy, causing attributes to be lost or shared.

**Evidence from usage:**
```go
// logger/provider.go wraps tint handler
handler = NewContextHandler(handler)  // Context values propagate
// But if replacement's WithAttrs returns self, .With() calls silently fail
```

**Consequences:**
- `logger.With("request_id", rid).Info("msg")` loses request_id
- Nested groups create unexpected output structure
- Debugging becomes impossible when context is lost

**Warning signs:**
- Log output missing expected attributes
- Tests for contextual logging fail
- `.With()` calls have no effect

**Prevention:**
```go
func (h *ColorHandler) WithAttrs(attrs []slog.Attr) slog.Handler {
    h2 := *h // Shallow copy
    h2.attrs = append(slices.Clone(h.attrs), attrs...)
    return &h2 // Return NEW handler
}

func (h *ColorHandler) WithGroup(name string) slog.Handler {
    h2 := *h
    h2.groups = append(slices.Clone(h.groups), name)
    return &h2
}
```

**Phase to address:** Phase 3 (tint replacement) - Add tests for `.With()` preservation

---

### Pitfall 6: Cron SkipIfStillRunning Semantics

**What goes wrong:** The current scheduler uses `cron.SkipIfStillRunning` to prevent overlapping executions. The internal implementation may not have this wrapper.

**Current behavior:**
```go
// cron/scheduler.go
c := cron.New(
    cron.WithChain(cron.SkipIfStillRunning(adapter)),
)
```

If a job scheduled for `*/5 * * * *` (every 5 min) takes 6 minutes, the next execution is skipped. Without this wrapper, jobs queue up.

**Consequences:**
- Job queue grows unbounded during slow execution
- Memory exhaustion from queued jobs
- "Thundering herd" when backlog executes rapidly

**Warning signs:**
- Memory usage grows over time
- Jobs execute multiple times in rapid succession
- CPU spikes after job completes

**Prevention:**
```go
// Internal cron/internal must include equivalent chain wrapper
func SkipIfStillRunning(j Job) Job {
    var running atomic.Bool
    return FuncJob(func() {
        if running.Swap(true) {
            logger.Info("job still running, skipping")
            return
        }
        defer running.Store(false)
        j.Run()
    })
}
```

**Phase to address:** Phase 1 (cron/internal replacement) - Verify Chain API compatibility

---

### Pitfall 7: Health Check Timeout Propagation

**What goes wrong:** Health checks that depend on external services (DB ping, Redis) may timeout. The internal implementation must handle context timeouts correctly.

**Current pattern:**
```go
// _tmp_trust/healthz/check.go
func DatabasePingCheck(database *sql.DB, timeout time.Duration) Check {
    return func() error {
        ctx, cancel := context.WithTimeout(context.Background(), timeout)
        defer cancel()
        return database.PingContext(ctx)
    }
}
```

Note: Uses `context.Background()`, not propagated context. This is intentional but differs from the `alexliesenfeld/health` pattern which may propagate request context.

**Consequences:**
- Health checks hang if timeout not enforced
- Request context cancellation doesn't stop health check
- Kubernetes probe timeout reached before check timeout

**Warning signs:**
- Health endpoint responds after Kubernetes timeout (probe failure)
- Database connection pool exhausted during health checks
- Slow health checks under load

**Prevention:**
1. Ensure internal timeout < Kubernetes probe timeout
2. Add tests for timeout behavior:
```go
func TestHealthCheckTimeout(t *testing.T) {
    check := TimeoutCheck(500*time.Millisecond, func() error {
        time.Sleep(2 * time.Second) // Would hang without timeout
        return nil
    })
    
    start := time.Now()
    err := check()
    elapsed := time.Since(start)
    
    assert.Error(t, err, "should timeout")
    assert.Less(t, elapsed, 1*time.Second, "should not wait full 2s")
}
```

**Phase to address:** Phase 2 (health replacement) - Document timeout model

---

### Pitfall 8: ANSI Colors in Non-TTY Output

**What goes wrong:** The tint replacement outputs ANSI escape codes. If logs are piped to a file or Kubernetes logs, the output contains `^[[31m` garbage.

**Current usage:**
```go
// logger/provider.go
if cfg.Format == "text" {
    handler = tint.NewHandler(os.Stdout, &tint.Options{...})
}
```

Production typically uses `Format: "json"`, but development/debugging uses text. If the replacement doesn't detect TTY, logs become unreadable when redirected.

**Consequences:**
- Log files contain escape codes
- `kubectl logs` shows raw escape sequences
- Log aggregators (Loki, etc.) index garbage characters

**Warning signs:**
- Logs look fine in terminal, garbled when piped to file
- `./app > log.txt` creates unreadable output
- Docker logs contain `[31m` strings

**Prevention:**
```go
import "github.com/mattn/go-isatty"

func NewColorHandler(w io.Writer, opts *Options) *ColorHandler {
    h := &ColorHandler{w: w, opts: opts}
    
    // Disable colors for non-TTY output
    if f, ok := w.(*os.File); ok {
        h.noColor = !isatty.IsTerminal(f.Fd())
    } else {
        h.noColor = true // Not a file, assume no TTY
    }
    return h
}
```

**Phase to address:** Phase 3 (tint replacement) - Add TTY detection, or verify tint compatibility

---

## Minor Pitfalls

Mistakes that cause annoyance but are easily fixable.

---

### Pitfall 9: Cron Parser Versioning (5 vs 6 fields)

**What goes wrong:** `robfig/cron/v3` uses 5-field expressions by default (no seconds). The internal cron/internal may default to 6-field or different descriptors.

**Current usage:**
```go
// cron/scheduler.go
_, err := s.cron.AddJob(schedule, wrapper) // Uses default parser
```

**Consequences:**
- Existing `"0 * * * *"` (hourly) schedules fail to parse
- Or: `"0 0 * * * *"` (6-field) silently misinterpreted

**Prevention:**
- Document parser default clearly
- Add migration test with all existing schedule formats:
```go
func TestParseExistingSchedules(t *testing.T) {
    schedules := []string{
        "0 * * * *",     // hourly
        "*/5 * * * *",   // every 5 min
        "@hourly",       // descriptor
        "@daily",
    }
    for _, s := range schedules {
        _, err := internal.ParseStandard(s)
        assert.NoError(t, err, "schedule: %s", s)
    }
}
```

**Phase to address:** Phase 1 (cron/internal replacement) - Verify 5-field default

---

### Pitfall 10: DOW/DOM Semantic Difference

**What goes wrong:** The standard cron behavior for Day-of-Week and Day-of-Month interaction is OR (either matches), not AND. The internal implementation may differ.

**Standard behavior:**
```
"0 0 15 * Sun"  # Runs on the 15th OR any Sunday
"0 0 15 * *"    # Runs only on the 15th (DOW is *)
"0 0 * * Sun"   # Runs only on Sundays (DOM is *)
```

**Evidence from tests:**
```go
// _tmp_trust/cron/internal/spec_test.go
{\"Sun Jul 15 00:00 2012\", \"* * 1,15 * Sun\", true}, // Both restricted: OR
{\"Sun Jul 15 00:00 2012\", \"* * * * Mon\", false},   // DOW * and day: AND
```

**Consequences:**
- Jobs run on unexpected days
- "Every 15th that's also a Sunday" vs "Every 15th OR Sunday"

**Prevention:**
- Inherit tests from robfig/cron for DOM/DOW semantics
- Document the OR behavior explicitly

**Phase to address:** Phase 1 (cron/internal replacement) - Port robfig/cron test cases

---

## Integration Pitfalls (Existing System)

---

### Pitfall 11: Breaking Existing Test Mocking

**What goes wrong:** The 92.9% test coverage relies on mocking interfaces like `*backoff.Backoff`. Changing the type breaks existing tests.

**Current interface:**
```go
// worker/supervisor.go
type supervisor struct {
    backoff *backoff.Backoff  // Concrete type from jpillora/backoff
}
```

If replacement uses a different type, all tests using `supervisor` need updates.

**Prevention:**
1. Define internal interface first:
```go
type BackoffStrategy interface {
    Duration() time.Duration
    Reset()
}
```

2. Wrapper adapts both old and new:
```go
type backoffAdapter struct {
    inner *backoff.Backoff // or internal type
}
func (b *backoffAdapter) Duration() time.Duration { return b.inner.Duration() }
```

**Phase to address:** All phases - Define interfaces BEFORE replacing implementations

---

### Pitfall 12: Metric/Log Attribute Changes

**What goes wrong:** Replacing health library may change metric names or log attributes, breaking dashboards.

**Current observability:**
```go
// Health results include timestamps, status enums, check names
result.Details["database"].Status == health.StatusUp
```

If internal implementation uses different status values or field names, Grafana dashboards fail.

**Prevention:**
- Document all exported field names before replacement
- Keep same metric/attribute names or update dashboards atomically

**Phase to address:** Phase 2 (health replacement) - Audit observability integration

---

## Phase-Specific Warnings

| Phase | Topic | Likely Pitfall | Priority |
|-------|-------|----------------|----------|
| 1 | jpillora/backoff | Overflow handling, jitter races | HIGH |
| 1 | robfig/cron | DST edge cases, parser fields | CRITICAL |
| 2 | alexliesenfeld/health | Status code behavior, IETF format | CRITICAL |
| 3 | lmittmann/tint | WithAttrs/WithGroup, TTY detection | MEDIUM |

---

## Pre-Flight Checklist

Before replacing each dependency:

- [ ] Write characterization tests capturing current behavior
- [ ] Run tests with `go test -race` for concurrency issues
- [ ] Test in UTC and America/New_York timezones
- [ ] Verify log output format matches
- [ ] Check health endpoints return exact same status codes
- [ ] Confirm no metric/log attribute names changed
- [ ] Test redirect output to file (no ANSI codes)
- [ ] Validate DST transition behavior (March and November dates)
- [ ] Run full test suite with 92.9% coverage maintained

---

## Sources

| Source | Confidence |
|--------|------------|
| Code analysis of gaz/worker/*, gaz/cron/*, gaz/health/* | HIGH |
| Code analysis of _tmp_trust/cron/internal/*, _tmp_trust/healthz/* | HIGH |
| robfig/cron GitHub issues #157, #144 | HIGH |
| Web research on DST handling, backoff pitfalls | MEDIUM |
| Kubernetes health check best practices | HIGH |
